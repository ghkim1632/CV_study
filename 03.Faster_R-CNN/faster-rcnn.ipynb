{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster R-CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://github.com/chenyuntc/simple-faster-rcnn-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from __future__ import  absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.ops import nms\n",
    "from torch.utils import data as data_\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "import six\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import ipdb\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "# from utils.config import opt\n",
    "# from data.dataset import Dataset, TestDataset, inverse_normalize\n",
    "# from utils import array_tool as at\n",
    "# from utils.vis_tool import visdom_bbox\n",
    "import skimage\n",
    "from torchnet.meter import ConfusionMeter, AverageValueMeter\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "    opt._parse(kwargs)\n",
    "\n",
    "    # data loader\n",
    "    dataset = Dataset(opt)\n",
    "    dataloader = data_.DataLoader(dataset, \\\n",
    "                                  batch_size=1, \\\n",
    "                                  shuffle=True, \\\n",
    "                                  # pin_memory=True,\n",
    "                                  num_workers=opt.num_workers)\n",
    "    testset = TestDataset(opt)\n",
    "    test_dataloader = data_.DataLoader(testset,\n",
    "                                       batch_size=1,\n",
    "                                       num_workers=opt.test_num_workers,\n",
    "                                       shuffle=False, \\\n",
    "                                       pin_memory=True\n",
    "                                       )\n",
    "    # model construction\n",
    "    faster_rcnn = FasterRCNNVGG16()\n",
    "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
    "    if opt.load_path:\n",
    "        trainer.load(opt.load_path)\n",
    "    lr_ = opt.lr\n",
    "    for epoch in range(opt.epoch):\n",
    "        trainer.reset_meters()\n",
    "        for ii, (img, bbox_, label_, scale) in tqdm(enumerate(dataloader)):\n",
    "            scale = at.scalar(scale)\n",
    "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "            trainer.train_step(img, bbox, label, scale)\n",
    "\n",
    "            if (ii + 1) % opt.plot_every == 0:\n",
    "                if os.path.exists(opt.debug_file):\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "                # plot loss\n",
    "                trainer.vis.plot_many(trainer.get_meter_data())\n",
    "\n",
    "                # plot groud truth bboxes\n",
    "                ori_img_ = inverse_normalize(at.tonumpy(img[0]))\n",
    "                gt_img = visdom_bbox(ori_img_,\n",
    "                                     at.tonumpy(bbox_[0]),\n",
    "                                     at.tonumpy(label_[0]))\n",
    "                trainer.vis.img('gt_img', gt_img)\n",
    "\n",
    "                # plot predicti bboxes\n",
    "                _bboxes, _labels, _scores = trainer.faster_rcnn.predict([ori_img_], visualize=True)\n",
    "                pred_img = visdom_bbox(ori_img_,\n",
    "                                       at.tonumpy(_bboxes[0]),\n",
    "                                       at.tonumpy(_labels[0]).reshape(-1),\n",
    "                                       at.tonumpy(_scores[0]))\n",
    "                trainer.vis.img('pred_img', pred_img)\n",
    "\n",
    "                # rpn confusion matrix(meter)\n",
    "                trainer.vis.text(str(trainer.rpn_cm.value().tolist()), win='rpn_cm')\n",
    "                # roi confusion matrix\n",
    "                trainer.vis.img('roi_cm', at.totensor(trainer.roi_cm.conf, False).float())\n",
    "        eval_result = eval(test_dataloader, faster_rcnn, test_num=opt.test_num)\n",
    "        trainer.vis.plot('test_map', eval_result['map'])\n",
    "        lr_ = trainer.faster_rcnn.optimizer.param_groups[0]['lr']\n",
    "        log_info = 'lr:{}, map:{},loss:{}'.format(str(lr_),\n",
    "                                                  str(eval_result['map']),\n",
    "                                                  str(trainer.get_meter_data()))\n",
    "        trainer.vis.log(log_info)\n",
    "\n",
    "        if eval_result['map'] > best_map:\n",
    "            best_map = eval_result['map']\n",
    "            best_path = trainer.save(best_map=best_map)\n",
    "        if epoch == 9:\n",
    "            trainer.load(best_path)\n",
    "            trainer.faster_rcnn.scale_lr(opt.lr_decay)\n",
    "            lr_ = lr_ * opt.lr_decay\n",
    "\n",
    "        if epoch == 13: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import RoIPool\n",
    "\n",
    "# FasterRCNNVGG16\n",
    "\n",
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    model = vgg16(True)\n",
    "\n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    # delete dropout\n",
    "    del classifier[5]\n",
    "    del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier\n",
    "    \n",
    "\n",
    "class VGG16RoIHead(nn.Module):\n",
    "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)\n",
    "        self.score = nn.Linear(4096, n_class)\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "\n",
    "        self.n_class = n_class\n",
    "        self.roi_size = roi_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.roi = RoIPool((self.roi_size, self.roi_size), self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        roi_indices = at.totensor(roi_indices).float()\n",
    "        rois = at.totensor(rois).float()\n",
    "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois = xy_indices_and_rois.contiguous()\n",
    "\n",
    "        pool = self.roi(x, indices_and_rois)\n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        fc7 = self.classifier(pool)\n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        roi_scores = self.score(fc7)\n",
    "        return roi_cls_locs, roi_scores\n",
    "\n",
    "\n",
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "    def __init__(self,\n",
    "                 n_fg_class=20,\n",
    "                 ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32]):\n",
    "        extractor, classifier = decom_vgg16()\n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        )\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)\n",
    "    ):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor\n",
    "        self.rpn = rpn\n",
    "        self.head = head\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset('evaluate')\n",
    "    \n",
    "    @property\n",
    "    def n_class(self):\n",
    "        # Total number of classes including the background.\n",
    "        return self.head.n_class\n",
    "    \n",
    "    def forward(self, x, scale=1.):\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor =\\\n",
    "            self.rpn(h, img_size, scale)\n",
    "        roi_cls_locs, roi_scores =\\\n",
    "            self.head(h, rois, roi_indices)\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
    "    \n",
    "    def use_preset(self, preset):\n",
    "        if preset == 'visualize':\n",
    "            self.nms_thresh = 0.3\n",
    "            self.score_thresh = 0.7\n",
    "        elif preset == 'evaluate':\n",
    "            self.nms_thresh = 0.3\n",
    "            self.score_thresh = 0.05\n",
    "        else:\n",
    "            raise ValueError('preset must be visualize or evaluate')\n",
    "    \n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[: l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = nms(cls_bbox_l, prob_l, self.nms_thresh)\n",
    "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep].cpu().numpy())\n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.float32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score \n",
    "    \n",
    "    def predict(self, imgs, sizes=None, visualize=False):\n",
    "        self.eval()\n",
    "        if visualize:\n",
    "            self.use_preset('visualize')\n",
    "            prepared_imgs = list()\n",
    "            sizes = list()\n",
    "            for img in imgs:\n",
    "                size = img.shape[1:]\n",
    "                img = preprocess(at.tonumpy(img))\n",
    "                prepared_imgs.append(img)\n",
    "                sizes.append(size)\n",
    "        else:\n",
    "            prepared_imgs = imgs\n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = at.totensor(img[None]).float()\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale)\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = at.totensor(rois) / scale\n",
    "\n",
    "            mean = t.Tensor(self.loc_normalize_mean).cuda(). \\\n",
    "                repeat(self.n_class)[None]\n",
    "            std = t.Tensor(self.loc_normalize_std).cuda(). \\\n",
    "                repeat(self.n_class)[None]\n",
    "            \n",
    "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(at.tonumpy(roi).reshape((-1, 4)),\n",
    "                                at.tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
    "            cls_bbox = at.totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = (F.softmax(at.totensor(roi_score), dim=1))\n",
    "\n",
    "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(labels)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_present('evaluate')\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "    \n",
    "    def get_optimizer(self):\n",
    "        lr = opt.lr\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key:\n",
    "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': lr, 'weight_decay': opt.weight_decay}]\n",
    "        if opt.use_adam:\n",
    "            self.optimizer = t.optim.Adam(params)\n",
    "        else:\n",
    "            self.optimizer = t.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "        \n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= decay\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FasterRCNNTrainer(nn.Module):\n",
    "    def __init__(self, faster_rcnn):\n",
    "        super(FasterRCNNTrainer, self).__init__()\n",
    "        self.faster_rcnn = faster_rcnn\n",
    "        self.rpn_sigma = opt.rpn_sigma\n",
    "        self.roi_sigma = opt.roi_sigma\n",
    "\n",
    "        self.anchor_target_creator = AnchorTargetCreator()\n",
    "        self.proposal_target_creator = ProposalTargetCreator()\n",
    "\n",
    "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
    "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
    "\n",
    "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
    "        self.vis = Visualizer(env=opt.env)\n",
    "\n",
    "        self.rpn_cm = ConfusionMeter(2)\n",
    "        self.roi_cm = ConfusionMeter(21)\n",
    "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels, scale):\n",
    "        n = bboxes.shape[0]\n",
    "        if n != 1:\n",
    "            raise ValueError('Currently only batch size 1 is supported.')\n",
    "        \n",
    "        _, _, H, W = imgs.shape\n",
    "        img_size = (H, W)\n",
    "\n",
    "        features = self.faster_rcnn.extractor(imgs)\n",
    "\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n",
    "            self.faster_rcnn.rpn(features, img_size, scale)\n",
    "        \n",
    "        bbox = bboxes[0]\n",
    "        label = labels[0]\n",
    "        rpn_score = rpn_scores[0]\n",
    "        rpn_loc = rpn_locs[0]\n",
    "        roi = rois\n",
    "\n",
    "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
    "            roi,\n",
    "            at.tonumpy(bbox),\n",
    "            at.tonumpy(label),\n",
    "            self.loc_normalize_mean,\n",
    "            self.loc_normalize_std)\n",
    "        \n",
    "        sample_roi_index = torch.zeros(len(sample_roi))\n",
    "        roi_cls_loc, roi_score = self.faster_rcnn.head(\n",
    "            features,\n",
    "            sample_roi,\n",
    "            sample_roi_index)\n",
    "        \n",
    "        # ------------------ RPN losses -------------------# \n",
    "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(\n",
    "            at.tonumpy(bbox),\n",
    "            anchor,\n",
    "            img_size)\n",
    "        gt_rpn_label = at.totensor(gt_rpn_label).long()\n",
    "        gt_rpn_loc = at.totensor(gt_rpn_loc)\n",
    "        rpn_loc_loss = _fast_rcnn_loc_loss(\n",
    "            rpn_loc,\n",
    "            gt_rpn_loc,\n",
    "            gt_rpn_label.data,\n",
    "            self.rpn_sigma)\n",
    "        \n",
    "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
    "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
    "        _rpn_score = at.tonumpy(rpn_score)[at.tonumpy(gt_rpn_label) > -1]\n",
    "        self.rpn_cm.add(at.totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
    "\n",
    "        \n",
    "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
    "        n_sample = roi_cls_loc.shape[0]\n",
    "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
    "                              at.totensor(gt_roi_label).long()]\n",
    "        gt_roi_label = at.totensor(gt_roi_label).long()\n",
    "        gt_roi_loc = at.totensor(gt_roi_loc)\n",
    "\n",
    "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
    "            roi_loc.contiguous(),\n",
    "            gt_roi_loc,\n",
    "            gt_roi_label.data,\n",
    "            self.roi_sigma)\n",
    "        \n",
    "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
    "\n",
    "        self.roi_cm.add(at.totensor(roi_score, False), gt_roi_label.data.long())\n",
    "\n",
    "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
    "        losses = losses + [sum(losses)]\n",
    "\n",
    "        return LossTuple(*losses)\n",
    "    \n",
    "    def train_step(self, imgs, bboxes, labels, scale):\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = self.forward(imgs, bboxes, labels, scale)\n",
    "        losses.total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_meters(losses)\n",
    "        return losses\n",
    "    \n",
    "\n",
    "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
    "    sigma2 = sigma ** 2\n",
    "    diff = in_weight * (x - t)\n",
    "    abs_diff = diff.abs()\n",
    "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
    "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
    "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
    "    return y.sum()\n",
    "\n",
    "\n",
    "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
    "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
    "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
    "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
    "    loc_loss /= ((gt_label >= 0).sum().float()) # ignore gt_label==-1 for rpn_loss\n",
    "    return loc_loss\n",
    "\n",
    "\n",
    "class ProposalTargetCreator(object):\n",
    "    def __init__(self,\n",
    "                 n_sample=128,\n",
    "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
    "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
    "                 ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh_hi = neg_iou_thresh_hi\n",
    "        self.neg_iou_thresh_lo = neg_iou_thresh_lo\n",
    "    \n",
    "    def __call__(self, roi, bbox, label,\n",
    "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
    "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
    "        n_bbox, _ = bbox.shape\n",
    "\n",
    "        roi = np.concatenate((roi, bbox), axis=0)\n",
    "\n",
    "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio)\n",
    "        iou = bbox_iou(roi, bbox)\n",
    "        gt_assignment = iou.argmax(axis=1)\n",
    "        max_iou = iou.max(axis=1)\n",
    "        # Offset range of classes from [0, n_fg_class - 1] to [1, n_fg_class].\n",
    "        # The label with value 0 is the background.\n",
    "        gt_roi_label = label[gt_assignment] + 1\n",
    "\n",
    "        # Select foreground RoIs as those with >= pos_iou_thresh IoU.\n",
    "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "        if pos_index.size > 0:\n",
    "            pos_index = np.random.choice(\n",
    "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "\n",
    "        # Select background RoIs as those within\n",
    "        # [neg_iou_thresh_lo, neg_iou_thresh_hi).\n",
    "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
    "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
    "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
    "                                         neg_index.size))\n",
    "        if neg_index.size > 0:\n",
    "            neg_index = np.random.choice(\n",
    "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "\n",
    "        # The indices that we're selecting (both positive and negative).\n",
    "        keep_index = np.append(pos_index, neg_index)\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
    "        sample_roi = roi[keep_index]\n",
    "\n",
    "        # Compute offsets and scales to match sampled RoIs to the GTs.\n",
    "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]])\n",
    "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)\n",
    "                       ) / np.array(loc_normalize_std, np.float32))\n",
    "\n",
    "        return sample_roi, gt_roi_loc, gt_roi_label\n",
    "    \n",
    "    \n",
    "class AnchorTargetCreator(object):\n",
    "    def __init__(self,\n",
    "                 n_sample=256,\n",
    "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
    "                 pos_ratio=0.5):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh = neg_iou_thresh\n",
    "        self.pos_ratio = pos_ratio\n",
    "\n",
    "    def __call__(self, bbox, anchor, img_size):\n",
    "        img_H, img_W = img_size\n",
    "\n",
    "        n_anchor = len(anchor)\n",
    "        inside_index = _get_inside_index(anchor, img_H, img_W)\n",
    "        anchor = anchor[inside_index]\n",
    "        argmax_ious, label = self._create_label(\n",
    "            inside_index, anchor, bbox)\n",
    "\n",
    "        # compute bounding box regression targets\n",
    "        loc = bbox2loc(anchor, bbox[argmax_ious])\n",
    "\n",
    "        # map up to original set of anchors\n",
    "        label = _unmap(label, n_anchor, inside_index, fill=-1)\n",
    "        loc = _unmap(loc, n_anchor, inside_index, fill=0)\n",
    "\n",
    "        return loc, label\n",
    "\n",
    "    def _create_label(self, inside_index, anchor, bbox):\n",
    "        # label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
    "        label.fill(-1)\n",
    "\n",
    "        argmax_ious, max_ious, gt_argmax_ious = \\\n",
    "            self._calc_ious(anchor, bbox, inside_index)\n",
    "\n",
    "        # assign negative labels first so that positive labels can clobber them\n",
    "        label[max_ious < self.neg_iou_thresh] = 0\n",
    "\n",
    "        # positive label: for each gt, anchor with highest iou\n",
    "        label[gt_argmax_ious] = 1\n",
    "\n",
    "        # positive label: above threshold IOU\n",
    "        label[max_ious >= self.pos_iou_thresh] = 1\n",
    "\n",
    "        # subsample positive labels if we have too many\n",
    "        n_pos = int(self.pos_ratio * self.n_sample)\n",
    "        pos_index = np.where(label == 1)[0]\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(\n",
    "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        # subsample negative labels if we have too many\n",
    "        n_neg = self.n_sample - np.sum(label == 1)\n",
    "        neg_index = np.where(label == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(\n",
    "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        return argmax_ious, label\n",
    "    \n",
    "    def _calc_ious(self, anchor, bbox, inside_index):\n",
    "        # ious between the anchors and the gt boxes\n",
    "        ious = bbox_iou(anchor, bbox)\n",
    "        argmax_ious = ious.argmax(axis=1)\n",
    "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
    "        gt_argmax_ious = ious.argmax(axis=0)\n",
    "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "        return argmax_ious, max_ious, gt_argmax_ious"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_normalize(img):\n",
    "    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                 std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(torch.from_numpy(img))\n",
    "    return img.numpy()\n",
    "\n",
    "\n",
    "def preprocess(img, min_size=600, max_size=1000):\n",
    "    C, H, W = img.shape\n",
    "    scale1 = min_size / min(H, W)\n",
    "    scale2 = max_size / max(H, W)\n",
    "    img = img / 255\n",
    "    img = skimage.transforms.resize(img, (C, H * scale, W * scale), mode='reflect', anti_aliasing=False)\n",
    "    return pytorch_normalize(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "            anchor_scales=[8, 16, 32], feat_stride=16,\n",
    "            proposal_creator_params=dict(),\n",
    "    ):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.anchor_base = generate_anchor_base(\n",
    "            anchor_scales=anchor_scales, ratios=ratios)\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n",
    "        n_anchor = self.anchor_base.shape[0]\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n",
    "        normal_init(self.conv1, 0, 0.01)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "        normal_init(self.loc, 0, 0.01)\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        n, _, hh, ww = x.shape\n",
    "        anchor = _enumerate_shifted_anchor(\n",
    "            np.array(self.anchor_base),\n",
    "            self.feat_stride, hh, ww)\n",
    "\n",
    "        n_anchor = anchor.shape[0] // (hh * ww)\n",
    "        h = F.relu(self.conv1(x))\n",
    "\n",
    "        rpn_locs = self.loc(h)\n",
    "        # UNNOTE: check whether need contiguous\n",
    "        # A: Yes\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "        rpn_scores = self.score(h)\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2)\n",
    "\n",
    "        rois = list()\n",
    "        roi_indices = list()\n",
    "        for i in range(n):\n",
    "            roi = self.proposal_layer(\n",
    "                rpn_locs[i].cpu().data.numpy(),\n",
    "                rpn_fg_scores[i].cpu().data.numpy(),\n",
    "                anchor, img_size,\n",
    "                scale=scale)\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()    \n",
    "\n",
    "\n",
    "class ProposalCreator:\n",
    "    def __init__(self,\n",
    "                 parent_model,\n",
    "                 nms_thresh=0.7,\n",
    "                 n_train_pre_nms=12000,\n",
    "                 n_train_post_nms=2000,\n",
    "                 n_test_pre_nms=6000,\n",
    "                 n_test_post_nms=300,\n",
    "                 min_size=16\n",
    "                 ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "    \n",
    "    def __call__(self, loc, score, \n",
    "                 anchor, img_size, scale=1.):\n",
    "        if self.parent_model.training:\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(\n",
    "            roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "        \n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "        score = score[order]\n",
    "\n",
    "        keep = nms(\n",
    "            torch.from_numpy(roi).cuda(),\n",
    "            torch.from_numpy(score).cuda(),\n",
    "            self.nms_thresh)\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep.cpu().numpy()]\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataloader, faster_rcnn, test_num=10000):\n",
    "    pred_bboxes, pred_labels, pred_scores = list(), list(), list()\n",
    "    gt_bboxes, gt_labels, gt_difficults = list(), list(), list()\n",
    "    for ii, (imgs, sizes, gt_bboxes_, gt_labels_, gt_difficults_) in tqdm(enumerate(dataloader)):\n",
    "        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n",
    "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
    "        gt_bboxes += list(gt_bboxes_.numpy())\n",
    "        gt_labels += list(gt_labels_.numpy())\n",
    "        gt_difficults += list(gt_difficults_.numpy())\n",
    "        pred_bboxes += pred_bboxes_\n",
    "        pred_labels += pred_labels_\n",
    "        pred_scores += pred_scores_\n",
    "        if ii == test_num: break\n",
    "\n",
    "    result = eval_detection_voc(\n",
    "        pred_bboxes, pred_labels, pred_scores,\n",
    "        gt_bboxes, gt_labels, gt_difficults,\n",
    "        use_07_metric=True)\n",
    "    return \n",
    "\n",
    "\n",
    "def eval_detection_voc(\n",
    "        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n",
    "        gt_difficults=None,\n",
    "        iou_thresh=0.5, use_07_metric=False):\n",
    "    prec, rec = calc_detection_voc_prec_rec(\n",
    "        pred_bboxes, pred_labels, pred_scores,\n",
    "        gt_bboxes, gt_labels, gt_difficults,\n",
    "        iou_thresh=iou_thresh)\n",
    "\n",
    "    ap = calc_detection_voc_ap(prec, rec, use_07_metric=use_07_metric)\n",
    "\n",
    "    return {'ap': ap, 'map': np.nanmean(ap)}\n",
    "\n",
    "\n",
    "def calc_detection_voc_prec_rec(\n",
    "        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n",
    "        gt_difficults=None,\n",
    "        iou_thresh=0.5):\n",
    "    pred_bboxes = iter(pred_bboxes)\n",
    "    pred_labels = iter(pred_labels)\n",
    "    pred_scores = iter(pred_scores)\n",
    "    gt_bboxes = iter(gt_bboxes)\n",
    "    gt_labels = iter(gt_labels)\n",
    "    if gt_difficults is None:\n",
    "        gt_difficults = itertools.repeat(None)\n",
    "    else:\n",
    "        gt_difficults = iter(gt_difficults)\n",
    "\n",
    "    n_pos = defaultdict(int)\n",
    "    score = defaultdict(list)\n",
    "    match = defaultdict(list)\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label, gt_difficult in \\\n",
    "            six.moves.zip(\n",
    "                pred_bboxes, pred_labels, pred_scores,\n",
    "                gt_bboxes, gt_labels, gt_difficults):\n",
    "\n",
    "        if gt_difficult is None:\n",
    "            gt_difficult = np.zeros(gt_bbox.shape[0], dtype=bool)\n",
    "\n",
    "        for l in np.unique(np.concatenate((pred_label, gt_label)).astype(int)):\n",
    "            pred_mask_l = pred_label == l\n",
    "            pred_bbox_l = pred_bbox[pred_mask_l]\n",
    "            pred_score_l = pred_score[pred_mask_l]\n",
    "            # sort by score\n",
    "            order = pred_score_l.argsort()[::-1]\n",
    "            pred_bbox_l = pred_bbox_l[order]\n",
    "            pred_score_l = pred_score_l[order]\n",
    "\n",
    "            gt_mask_l = gt_label == l\n",
    "            gt_bbox_l = gt_bbox[gt_mask_l]\n",
    "            gt_difficult_l = gt_difficult[gt_mask_l]\n",
    "\n",
    "            n_pos[l] += np.logical_not(gt_difficult_l).sum()\n",
    "            score[l].extend(pred_score_l)\n",
    "\n",
    "            if len(pred_bbox_l) == 0:\n",
    "                continue\n",
    "            if len(gt_bbox_l) == 0:\n",
    "                match[l].extend((0,) * pred_bbox_l.shape[0])\n",
    "                continue\n",
    "\n",
    "            # VOC evaluation follows integer typed bounding boxes.\n",
    "            pred_bbox_l = pred_bbox_l.copy()\n",
    "            pred_bbox_l[:, 2:] += 1\n",
    "            gt_bbox_l = gt_bbox_l.copy()\n",
    "            gt_bbox_l[:, 2:] += 1\n",
    "\n",
    "            iou = bbox_iou(pred_bbox_l, gt_bbox_l)\n",
    "            gt_index = iou.argmax(axis=1)\n",
    "            # set -1 if there is no matching ground truth\n",
    "            gt_index[iou.max(axis=1) < iou_thresh] = -1\n",
    "            del iou\n",
    "\n",
    "            selec = np.zeros(gt_bbox_l.shape[0], dtype=bool)\n",
    "            for gt_idx in gt_index:\n",
    "                if gt_idx >= 0:\n",
    "                    if gt_difficult_l[gt_idx]:\n",
    "                        match[l].append(-1)\n",
    "                    else:\n",
    "                        if not selec[gt_idx]:\n",
    "                            match[l].append(1)\n",
    "                        else:\n",
    "                            match[l].append(0)\n",
    "                    selec[gt_idx] = True\n",
    "                else:\n",
    "                    match[l].append(0)\n",
    "\n",
    "    for iter_ in (\n",
    "            pred_bboxes, pred_labels, pred_scores,\n",
    "            gt_bboxes, gt_labels, gt_difficults):\n",
    "        if next(iter_, None) is not None:\n",
    "            raise ValueError('Length of input iterables need to be same.')\n",
    "\n",
    "    n_fg_class = max(n_pos.keys()) + 1\n",
    "    prec = [None] * n_fg_class\n",
    "    rec = [None] * n_fg_class\n",
    "\n",
    "    for l in n_pos.keys():\n",
    "        score_l = np.array(score[l])\n",
    "        match_l = np.array(match[l], dtype=np.int8)\n",
    "\n",
    "        order = score_l.argsort()[::-1]\n",
    "        match_l = match_l[order]\n",
    "\n",
    "        tp = np.cumsum(match_l == 1)\n",
    "        fp = np.cumsum(match_l == 0)\n",
    "\n",
    "        # If an element of fp + tp is 0,\n",
    "        # the corresponding element of prec[l] is nan.\n",
    "        prec[l] = tp / (fp + tp)\n",
    "        # If n_pos[l] is 0, rec[l] is None.\n",
    "        if n_pos[l] > 0:\n",
    "            rec[l] = tp / n_pos[l]\n",
    "\n",
    "    return prec, rec\n",
    "\n",
    "\n",
    "def calc_detection_voc_ap(prec, rec, use_07_metric=False):\n",
    "    n_fg_class = len(prec)\n",
    "    ap = np.empty(n_fg_class)\n",
    "    for l in six.moves.range(n_fg_class):\n",
    "        if prec[l] is None or rec[l] is None:\n",
    "            ap[l] = np.nan\n",
    "            continue\n",
    "\n",
    "        if use_07_metric:\n",
    "            # 11 point metric\n",
    "            ap[l] = 0\n",
    "            for t in np.arange(0., 1.1, 0.1):\n",
    "                if np.sum(rec[l] >= t) == 0:\n",
    "                    p = 0\n",
    "                else:\n",
    "                    p = np.max(np.nan_to_num(prec[l])[rec[l] >= t])\n",
    "                ap[l] += p / 11\n",
    "        else:\n",
    "            # correct AP calculation\n",
    "            # first append sentinel values at the end\n",
    "            mpre = np.concatenate(([0], np.nan_to_num(prec[l]), [0]))\n",
    "            mrec = np.concatenate(([0], rec[l], [1]))\n",
    "\n",
    "            mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n",
    "\n",
    "            # to calculate area under PR curve, look for points\n",
    "            # where X axis (recall) changes value\n",
    "            i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "            # and sum (\\Delta recall) * prec\n",
    "            ap[l] = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bbox_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc2bbox(src_bbox, loc):  # xywh -> xyxy\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):  # xyxy -> xywh\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc\n",
    "\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "\n",
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "    py = base_size / 2.\n",
    "    px = base_size / 2.\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n",
    "                           dtype=np.float32)\n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "            \n",
    "            index = i * len(anchor_scales) + j\n",
    "            anchor_base[index, 0] = py - h / 2.\n",
    "            anchor_base[index, 1] = px - w / 2.\n",
    "            anchor_base[index, 2] = py + h + 2.\n",
    "            anchor_base[index, 3] = px + w / 2.\n",
    "    return anchor_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Configs for training\n",
    "class Config:\n",
    "    # data\n",
    "    voc_data_dir = '/dataset/PASCAL2007/VOC2007/'\n",
    "    min_size = 600  # image resize\n",
    "    max_size = 1000 # image resize\n",
    "    num_workers = 8\n",
    "    test_num_workers = 8\n",
    "\n",
    "    # sigma for l1_smooth_loss\n",
    "    rpn_sigma = 3.\n",
    "    roi_sigma = 1.\n",
    "\n",
    "    # param for optimizer\n",
    "    # 0.0005 in origin paper but 0.0001 in tf-faster-rcnn\n",
    "    weight_decay = 0.0005\n",
    "    lr_decay = 0.1  # 1e-3 -> 1e-4\n",
    "    lr = 1e-3\n",
    "\n",
    "\n",
    "    # visualization\n",
    "    env = 'faster-rcnn'  # visdom env\n",
    "    port = 8097\n",
    "    plot_every = 40  # vis every N iter\n",
    "\n",
    "    # preset\n",
    "    data = 'voc'\n",
    "    pretrained_model = 'vgg16'\n",
    "\n",
    "    # training\n",
    "    epoch = 14\n",
    "\n",
    "\n",
    "    use_adam = False # Use Adam optimizer\n",
    "    use_chainer = False # try match everything as chainer\n",
    "    use_drop = False # use dropout in RoIHead\n",
    "    # debug\n",
    "    debug_file = '/tmp/debugf'\n",
    "\n",
    "    test_num = 10000\n",
    "    # model\n",
    "    load_path = None\n",
    "\n",
    "    caffe_pretrain = False # use caffe pretrained model instead of torchvision\n",
    "    caffe_pretrain_path = 'checkpoints/vgg16_caffe.pth'\n",
    "\n",
    "    def _parse(self, kwargs):\n",
    "        state_dict = self._state_dict()\n",
    "        for k, v in kwargs.items():\n",
    "            if k not in state_dict:\n",
    "                raise ValueError('UnKnown Option: \"--%s\"' % k)\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        print('======user config========')\n",
    "        pprint(self._state_dict())\n",
    "        print('==========end============')\n",
    "\n",
    "    def _state_dict(self):\n",
    "        return {k: getattr(self, k) for k, _ in Config.__dict__.items() \\\n",
    "                if not k.startswith('_')}\n",
    "\n",
    "\n",
    "opt = Config()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
